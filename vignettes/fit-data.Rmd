---
title: "Fit data and plot results"
author: "Andrew Edwards"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fit data and plot results}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
date: "Last rendered on `r format(Sys.time(), '%d %B, %Y')`"
---

```{r, echo = FALSE, eval = FALSE}
# To build either run this line or click knit button in RStudio
rmarkdown::render("fit-data.Rmd")
```

```{r, include = FALSE}
load_all()   # TODO replace with library(sizeSpectra2)

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 6
)
```

Fitting a size spectrum to a data set using likelihood is done using the
function `fit_size_spectrum()`. The function automatically selects the
method to use based on the class of the data, as described by Table 2 in our
[MEPS paper](https://www.int-res.com/abstracts/meps/v636/p19-33/).

TODO For README file. Let's have a simple vector of real measurements, and do
the fitting and plotting functions, without extra explanation. This will show
the simplicity, and refer to vignette for more details.

### Vector of individual measurements

The simplest data is just a vector  of measurements, such as body masses or
body lengths. We fit the size spectrum by calculating the maximum likelihood estimate
(the MLE method) of the size-spectrum exponent, along with its 95% confidence intervals.

The simulated data used from a bounded power-law distribution as used for Figures 1 and 2 of
our [MEE paper](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12641/full)
are included as the data object `sim_vec` (which stands for simulated vector) in
sizeSpectra2, and so we can just use that. Note that the fit will be good
because the data are sampled from the expected power-law distribution.

So let's print the first 15 values, check
the length is 1000, and do a quick histogram to show the long tail:
```{r, MLE1}
sim_vec[1:15]
length(sim_vec)
hist(sim_vec, breaks = 100)
```

To fit the data using the MLE method, simply use `fit_size_spectrum()`
```{r, MLE2}
res_vec <- fit_size_spectrum(sim_vec)
res_vec
```

So `res_vec` is a list that contains the MLE and confidence intervals for $b$,
as components `b_mle` and `b_conf` ,respectively,
plus also the original data as component `x`. When printing I have set it up to just show the
first 10 values of the original data. You can check that all 1000 values are in
`res_vec$x` with simply:
```{r, MLE3}
length(res_vec$x)
```

Now, to plot the PLB fit and the original data, simply do:
```{r, MLE4}
plot(res_vec)
```

The style of plot is as recommended in Figure 6b of
our [MEE paper](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12641/full), and
includes the PLB fits using the values of $b$ at the bounds of the confidence
interval for `b`.

The plot is as shown because we have written a function
`plot.size_spectrum_numeric()` which gets automatically used on `res_vec`
because `res_vec` has class `size_spectrum_numeric`, which we assigned
automatically in the function `fit_size_spectrum()` that was used above to
create `res_vec`. This all happens behind the scenes but it is useful for users
to know about this. In particular, to see the help you need to do
`?plot.size_spectrum_numeric()`. There you can see lots of customisable options
by setting arguments; for example to add the extra labels on the x-axis (to
*exactly* match the Figure 6b):
```{r, MLE5}
plot(res_vec,
     x_small_ticks_labels = c(5, 50, 500))
```

TODO After doing MLEbin below , then come back and add extra options for this one (since will first
create binned figures for MLEbin). TODO

### Binned data of counts in bins

Now we demonstrate the MLEbin method, which is for binned data. Let's just take
the same simulated data as used above, bin it, and then use the MLEbin
method. Then we will do an example with real data.

The function `bin_data()` takes a vector of values and assigns it to bins:
```{r, MLEbin1}
sim_vec_binned <- bin_data(sim_vec,
                           bin_width = "2k")
sim_vec_binned
```
The `bin_width = "2k"` argument says to assign bin widths that double in width.
The function returns a list of two tibbles, the first (`$bin_for_each_x` above) shows each
original value of `sim_vec`, the counts of that value (just 1 here because in
`sim_vec` we have individual values at a high level of resolution), and the bin that that value
is assigned to. The bins are defined by their midpoint, minimum, and maximum
values, given as `bin_mid`, `bin_min`, and `bin_max`, respectively. Given there
are many values close to 1 the above values are not overly instructive. To see
the higheest values:
```{r, MLEbin2}
tail(sim_vec_binned$bin_for_each_x)
```
to see the bins that the rare large values get assigned to.

The second component of the output of `bin_data()` is the tibble `bin_vals`, as
seen above. This just gives the counts in the bins (`bin_count`), along with
various useful the bin count divided by the bin width (`bin_count_norm`). See
`?bin_data` for full details of outputs. For your own binned data that might be
in a different format, we have some helper functions to get the data into the
desired format (see TODO below).

We will only used the binned data now (`$bin_vals`) without knowledge of the raw
measurements, to replicate what we might have for real data where measurements
are assigned to bins (in this case ones that double in size; real data would
more likely have linear bins based on the measurement accuracy of the
instruments).

TODO actually, just include a simple example data set of binned values. Move the
above to later.

TODO HERE Basically am redoing the MLEbin_recommend vignette but with more
usable functions.

NEXT: want `fit_size_spectrum()` to detect that we are applying it to a tibble,
and then apply MLEbin automatically. Need to decide on a standard for input. And
create a function to make into that if we have a vector of counts and binbreaks.

Got to HERE in `fit-size-spectrum.data.frame.R`, but need to :

[have moved works-in-progress files to R-temp/]
 - make a general `calc_mle_conf` function based on previous `calcLike` (see
   `fit-size-spectrum.numeric.R` also, and move some ideas from that into
   `calc_mle_conf()`, plus replace that code with `calc_mle_conf()`.). Need to make `prof_like()`.
 - use that in `fit-size-spectrum.numeric.R` instead of current
 - back to `fit-size-spectrum.data.frame.R` to carry on. See HERE.

And then also consider that we want to also allow for a tibble that has year or
another heading (maybe allow it to be called strata, or let user add that as an
option - yes, strata = year as the default, or maybe strata = NULL is the default).


our [MEE paper](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12641/full)

our [MEPS paper](https://www.int-res.com/abstracts/meps/v636/p19-33/)



`r knitr::knit_exit()`

This vignette reproduces the main results in the manuscript, and can easily be
used as a template for users to apply to their own data or model outputs. The longer
`results-extra` vignette gives more details of outputs and further examples of
plotting features.

Set up:
```{r setup}
library(hdiAnalysis)
library(dplyr)
```

## Analyse a single vector of values

Our first 'data' set is the hake Markov chain Monte Carlo (MCMC) recruitment
values for 2021, which is saved as a data object in the package as
`rec_2021`. It is a simple vector of the 8,000 MCMC values, and we will use it
to create Figure 1 and associated numbers. To run this code on your own
data you just need to have your values in a vector, and replace `rec_2021` with the
name of your vector.

```{r vector}
vec <- rec_2021
summary(vec)
length(vec)
```

We use the `create_intervals` function to do the main calculations. We call the
results here `res_vec` for results on a vector, so that the subsequent code will not need changing by users for their own analyses:

```{r vectorres}
res_vec <- create_intervals(vec)
res_vec
```
As seen above, `res_vec` is a list, with first element `$intervals` containing
results such as the median, lower and upper bounds for the equal-tailed interval
(ETI) and for the highest density interval (HDI) , widths of the
ETI and HDI, and more. See `?create_intervals` for full details of what is returned.
We will show a summary table of the key values after plotting the
figures.

The second element of `res_vec` is `$density` (seen above), which is a
kernel density estimate of the distribution represented by the samples in
`vec`. It is useful for plotting and calculating approximate intervals.

The HDI is calculated by `create_intervals()` using the `HDInterval::hdi()`
function, and we explore other choices in the `results-extra` vignette.

## Plot the distribution showing the ETI and HDI -- Figure 1

Plotting the resulting density function and intervals is simply done using our
custom plotting functions, with options for automatically adding intervals and
lines. The ETI plot (Figure 1A) is:
```{r plot, fig.height = 4}
plot(res_vec,
     type = "eti",
     xlim = c(0, 40),
     ylim = c(0, 0.11),
     interval_arrows = TRUE,
     xlab = "Recruitment (billions of age-0 fish)",
     arrowhead_gap = 0.1)
```
and the HDI plot (Figure 1B) is
```{r plot2, fig.height = 4}
plot(res_vec,
     type = "hdi",
     xlim = c(0, 40),
     ylim = c(0, 0.11),
     interval_arrows = TRUE,
     xlab = "Recruitment (billions of age-0 fish)",
     arrowhead_gap = 0.1)
```
The `plot()` command automatically uses our custom plotting function
`plot.intervals_density()`, because the `res_vec` object has our class
`intervals_density`. See `?plot.intervals_density()` for details and full
options. Our complete Figure 1
is reproduced with the command `figure_1()`, which can be adapted by users if
desired for their own data, or just adapt the above code as necessary
(e.g., change the limits and labels).

The main values of interest are simplified in our customised `summary_table()` function:
```{r tab, results = "asis"}
summary_table(res_vec)
```

The full results are
```{r fullres}
res_vec$intervals %>% a()       # a() is our shorthand function for as.data.frame()
```
for which definitions are given in the help file `?create_intervals`.


## MCMC samples for multiple years -- recruitment for Figure 2A

We now present calculations for multiple data sets, in our case multiple years
of MCMC samples of hake recruitment.

The values are saved in the package as the tibble `hake_recruitment_mcmc`, with each
column corresponding to a year
(with the first column representing the Virgin unfished equilibrium biomass) and
each of the 8,000 rows representing an MCMC sample:
```{r hake_mcmc}
hake_recruitment_mcmc
```

For your own data, wrangle it into the same kind of tibble, with rows
representing the MCMC samples.

To create ETIs and HDIs for estimates of recruitment for each year, we can simply use
`create_intervals()` which, because `hake_recruitment_mcmc` has class
`data.frame`, uses our function `create_intervals.data.frame()` to automatically
calculate intervals for each column (year in this case, and we will exclude the
Virgin column) of samples.
```{r hake_mcmc2}
res_all_years <- create_intervals(dplyr::select(hake_recruitment_mcmc,
                                                -"Virgin"))
```
This gives a list object, with element `res_all[[i]]` corresponding to column `i`
of the data. Each `res_all[[i]]` element is itself a list, giving the same
results as above for a single vector (and also with the `$name`
element corresponding the column of the input, a year in this example). So the
2021 results from above are:
```{r hake_mcmc3}
res_all_years$res_all[[56]]
```

For convenience, the intervals for all years are saved in a single tibble:
```{r hake_mcmc4}
res_all_years$intervals_all
```
The first column is called `quantity` for generalisability so the code can be
applied to different types of data; in this example it represents the years

To see the main values of interest, namely the ETIs and HDIs and the difference
between their widths:
```{r hake_mcmc5}
summary_table(res_all_years)
```

The sum of the differences over all years between the width of the ETI and width
of the HDI, (but excluding 2023 and 2024 as these are not informed by data for
our hake recruitment values), is
```{r hake_mcmc6}
res_all_years$intervals_all %>%
  dplyr::filter(!quantity %in% c(2023, 2024)) %>%
  dplyr::pull(width_diff) %>%
  sum()
```
which is the source of the ``>30 billion fish`` statement reported in the main text.

To plot the time series shown in Fig. 2A, we have a custom plotting function
`plot.intervals_density_list()`, that gets called because `res_all_years` is
defined to have class `intervals_density_list`, so we can just use:
```{r hake_mcmc7}
plot(res_all_years,
     xlab = "Year",
     ylab = "Recruitment (billions of fish)")
```

<!-- The ETIs match those shown in Table 24 of the 2024 hake assessment (values
manually checked, code is below but not run or printed in vignette): -->
```{r hake_mcmc8, eval = FALSE, echo = FALSE}
res_all_years$intervals_all %>%
  dplyr::select(eti_lower, eti_upper) %>%
  a() %>%
  round(3) * 1000
```

## MCMC samples for multiple years -- relative spawning biomass for Figure 2B

We now use the same approach for the relative spawning biomass calculations
shown in Figure 2B.

The female spawning biomass is the estimated total biomass of all females in the
population that are mature (for context, roughly half of age-2 females are considered
mature). The relative spawning biomass is the females spawning biomass divided
by that for the unfished equilibrium state (the calculation is already done for
each MCMC sample and then the ratios saved).

The values are saved in the package
as a tibble `hake_relative_biomass_mcmc` with years as column headings, and also
includes forecasts made assuming constant catches in 2024, 2025,
and 2026 of 350,000 t, which is close to the average coastwide catch from 2014-2023.
Values correspond to the relative spawning biomass at the start of the
corresponding year (before any fishing).

We can use similar code to above (since the values are in a tibble):
```{r allyears}
res_all_years_2 <- create_intervals(hake_relative_biomass_mcmc)

res_all_years_2$intervals_all

res_all_years_2$intervals_all %>% tail()
```

<!-- To confirm that the ETIs match those in Table b of the 2024 hake assessment -->
<!-- (which does not have 2025 onwards); keeping the code for reference -->

```{r allyearsmatch, echo = FALSE, eval = FALSE}
res_all_years_2$intervals_all %>%
  filter(quantity >= 2015) %>%
  mutate(`Year` = quantity,
         `Rel. SB 2.5th percentile` = eti_lower * 100,
         `Rel. SB median` = median * 100,
         `Rel. SB 97.5th percentile` = eti_upper * 100) %>%
  select(`Year`:`Rel. SB 97.5th percentile`) %>%
  filter(`Year` <= 2024) %>%
  knitr::kable(digits = 1)
```

To create Figure 2B, which includes the forecasts:
```{r biomassseries}
plot(res_all_years_2,
     xlim = c(2010, 2027),
     ylim = c(0, 2.6),
     add_line_at_0.4 = TRUE,
     inc = 0.05,
     leg_loc = "topleft",
     xlab = "Year",
     ylab = "Relative spawning biomass")
```

Whether the relative spawning biomass is above or below the management reference point
of 0.4 is key in forming stakeholders' perception of the health of the hake
stock. As seen in the figure, in 2024 and 2025 the ETI does not dip below 0.4 but
the HDI does. Thus, the perception of current and future stock status can depend upon the
definition of credible intervals (ETI versus HDI). The specific years across the
whole time series for which this happens are:

```{r whichyears}
dplyr::filter(res_all_years_2$intervals_all,
              eti_lower > 0.4,
              hdi_lower < 0.4)
```

Also, on the plot the years 2016 and 2020 look like they are also exhibiting this
behaviour, but in fact the HDI is very slightly above 0.4:
```{r whichyearsclose}
dplyr::filter(res_all_years_2$intervals_all,
              quantity %in% c(2016, 2020))
```

To see all the values of interest:
```{r summarytab2}
summary_table(res_all_years_2)
```

To plot results for a single year, for example 2024, extract the results
from the list and use our plotting function, and tailor the figure as necessary
by setting the arguments (see `?plot.intervals_density`):
```{r plotoneyear, fig.height = 10}
index_for_2024 <- which(res_all_years_2$intervals_all$quantity == 2024)
par(mfrow = c(2,1))
plot(res_all_years_2[["res_all"]][[index_for_2024]],
     type = "eti",
     xlim = c(0, 3.5),
     ylim = c(0, 1.2),
     interval_arrows = TRUE,
     xlab = "Relative spawning biomass in 2024",
     y_arrow = 1.07,
     arrowhead_gap = 0.01,
     x_minor_ticks_by = 0.25,
     y_minor_ticks_by = 0.05)

plot(res_all_years_2[["res_all"]][[index_for_2024]],
     type = "hdi",
     xlim = c(0, 3.5),
     ylim = c(0, 1.2),
     interval_arrows = TRUE,
     xlab = "Relative spawning biomass in 2024",
     y_arrow = 1.07,
     arrowhead_gap = 0.01,
     x_minor_ticks_by = 0.25,
     y_minor_ticks_by = 0.05)
```

## Absolute female spawning biomass

The above was for relative female spawning biomass. We now briefly look at
absolute female spawning biomass, in units of millions of tonnes. This is also
saved as a data object in the package (`hake_spawning_biomass_mcmc`). We focus
on the values
for 2023 and 2024 which are given prominence in a bullet in the one-page summary of the hake assessment:

"The median estimate of female spawning biomass at the start of 2024 is 1,884,950 t
(with 95% credible interval from 853,207 to 4,828,382 t). This is an upward shift
from this assessment’s estimate for the 2023 female spawning biomass of 1,335,485 t
(with 95% credible interval from 652,495 to 3,224,819 t)."

So those credible intervals are ETIs. How would HDIs compare? The results are
(in millions of tonnes):
```{r sb1}
res_all_years_spawning <-
  create_intervals(dplyr::select(hake_spawning_biomass_mcmc,
                                 `2023`,
                                 `2024`))

summary_table(res_all_years_spawning)
```
The above quote, using HDIs instead of ETIs, becomes:

"The median estimate of female spawning biomass at the start of 2024 is 1,884,950 t
(with 95% credible interval from
`r f(dplyr::filter(res_all_years_spawning$intervals_all, quantity ==
                                                       2024)$hdi_lower * 1e06)` to
`r f(dplyr::filter(res_all_years_spawning$intervals_all, quantity ==
                                                       2024)$hdi_upper * 1e06)` t).
This is an upward shift
from this assessment’s estimate for the 2023 female spawning biomass of 1,335,485 t
(with 95% credible interval from
`r f(dplyr::filter(res_all_years_spawning$intervals_all, quantity ==
                                                       2023)$hdi_lower * 1e06)` to
`r f(dplyr::filter(res_all_years_spawning$intervals_all, quantity ==
                                                       2023)$hdi_upper * 1e06)` t)."

The reduction in uncertainty for 2024 from switching from the ETI to the HDI is
`r f(dplyr::filter(res_all_years_spawning$intervals_all, quantity ==
                                                       2024)$width_diff * 1e06)` t.
This is larger than the average catch from 2014-2023 of 338,606 t, of which less
than half would be considered as spawning biomass (mature females; catch is
males plus females). Thus, the reduction in uncertainty is certainly not trivial.
Again, intervals are shifted downwards, implying the stock biomass is lower than implied
when using the original ETIs.

The ETI and HDI plots are
```{r,  sb2, fig.height = 10}
par(mfrow = c(2, 1))
plot(res_all_years_spawning$res_all[[which(res_all_years_spawning$intervals_all$quantity == 2024)]],
     xlim = c(0, 8),
     ylim = c(0, 0.6),
     interval_arrows = TRUE,
     y_arrow = 0.57,
     xlab = "Female spawning biomass in 2024 (millions of tonnes)",
     type = "eti")

plot(res_all_years_spawning$res_all[[which(res_all_years_spawning$intervals_all$quantity == 2024)]],
     xlim = c(0, 8),
     ylim = c(0, 0.6),
     interval_arrows = TRUE,
     y_arrow = 0.57,
     xlab = "Female spawning biomass in 2024 (millions of tonnes)",
     type = "hdi")
```

## Session information

```{r sessioninfo}
sessionInfo()
```
